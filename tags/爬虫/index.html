<!doctype html><html class=no-js lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>爬虫 - Linux.cn Archive</title>
<script>(function(e,t){e[t]=e[t].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="爬虫"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="/tags/%E7%88%AC%E8%99%AB/"><meta itemprop=name content="爬虫"><meta itemprop=description content><link rel=preconnect href=https://fonts.bunny.net crossorigin><link rel=dns-prefetch href=//fonts.bunny.net><link rel=dns-prefetch href=//fonts.bunny.net><link rel=stylesheet href="https://fonts.bunny.net/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel="shortcut icon" href=/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title="Linux.cn Archive" rel=home><div class="logo__item logo__text"><div class=logo__title>Linux.cn Archive</div></div></a></div><nav class=menu><button class=menu__btn aria-haspopup=true aria-expanded=false tabindex=0>
<span class=menu__btn-title tabindex=-1>Menu</span></button><ul class=menu__list><li class=menu__item><a class=menu__link href=/><span class=menu__text>Home</span></a></li><li class=menu__item><a class=menu__link href=/categories/><span class=menu__text>Categories</span></a></li><li class=menu__item><a class=menu__link href=/tags/><span class=menu__text>Tags</span></a></li><li class=menu__item><a class=menu__link href><span class=menu__text>RSS</span></a></li></ul></nav></div></header><div class="wrapper flex"><div class=primary><main class="main list" role=main><header class=main__header><h1 class=main__title>爬虫</h1></header><article class="list__item post"><figure class="list__thumbnail thumbnail"><a class=thumbnail__link href=/article-14086-1.html><img class=thumbnail__image src=https://img.linux.net.cn/data/attachment/album/202112/16/142118cmffvtfrmh1h3ufv.jpg.thumb.jpg alt="Python Beautiful Soup 刮取简易指南"></a></figure><header class=list__header><h2 class="list__title post__title"><a href=/article-14086-1.html rel=bookmark>Python Beautiful Soup 刮取简易指南</a></h2><div class="list__meta meta"><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2021-12-16T14:21:29Z>December 16, 2021</time></div></div></header><div class="content list__excerpt post__content clearfix">Python 中的 Beautiful Soup 库可以很方便的从网页中提取 HTML 内容。</div></article><article class="list__item post"><figure class="list__thumbnail thumbnail"><a class=thumbnail__link href=/article-10640-1.html><img class=thumbnail__image src=/data/attachment/album/201903/21/212936ctlppldn0ipqp8wt.jpg.thumb.jpg alt="x86 和 ARM 的 Python 爬虫速度对比"></a></figure><header class=list__header><h2 class="list__title post__title"><a href=/article-10640-1.html rel=bookmark>x86 和 ARM 的 Python 爬虫速度对比</a></h2><div class="list__meta meta"><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2019-03-21T21:29:00Z>March 21, 2019</time></div></div></header><div class="content list__excerpt post__content clearfix">Scrapy 在树莓派上面的性能并不差，或许这是 ARM 架构服务器的又一个成功例子？</div></article><article class="list__item post"><figure class="list__thumbnail thumbnail"><a class=thumbnail__link href=/article-10609-1.html><img class=thumbnail__image src=/data/attachment/album/201903/11/224237ba1adl8jd18mlady.jpg.thumb.jpg alt="使用 shell 构建多进程的 CommandlineFu 爬虫"></a></figure><header class=list__header><h2 class="list__title post__title"><a href=/article-10609-1.html rel=bookmark>使用 shell 构建多进程的 CommandlineFu 爬虫</a></h2><div class="list__meta meta"><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2019-03-11T22:45:00Z>March 11, 2019</time></div></div></header><div class="content list__excerpt post__content clearfix">CommandlineFu 是一个记录脚本片段的网站，每个片段都有对应的功能说明和对应的标签。我想要做的就是尝试用 shell 写一个多进程的爬虫把这些代码片段记录在一个 org 文件中。</div></article><article class="list__item post"><figure class="list__thumbnail thumbnail"><a class=thumbnail__link href=/article-9364-1.html><img class=thumbnail__image src=/data/attachment/album/201802/20/144220dz8isfp9ebsede8d.jpg.thumb.jpg alt=什么是网络爬虫？网络爬虫如何工作？></a></figure><header class=list__header><h2 class="list__title post__title"><a href=/article-9364-1.html rel=bookmark>什么是网络爬虫？网络爬虫如何工作？</a></h2><div class="list__meta meta"><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2018-02-20T14:42:16Z>February 20, 2018</time></div></div></header><div class="content list__excerpt post__content clearfix">作为一个狂热的互联网人，你在生活中一定遇到过网络爬虫（Web Crawler）这个词。那么什么是网络爬虫，谁使用网络爬虫？它是如何工作的？让我们在本文中讨论这些。</div></article><article class="list__item post"><figure class="list__thumbnail thumbnail"><a class=thumbnail__link href=/article-9030-1.html><img class=thumbnail__image src=https://img.linux.net.cn/data/attachment/album/201711/03/214537at1dsdt1tmoddaf2.png.thumb.jpg alt="三种 Python 网络内容抓取工具与爬虫"></a></figure><header class=list__header><h2 class="list__title post__title"><a href=/article-9030-1.html rel=bookmark>三种 Python 网络内容抓取工具与爬虫</a></h2><div class="list__meta meta"><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-11-03T21:45:33Z>November 03, 2017</time></div></div></header><div class="content list__excerpt post__content clearfix">运用这些很棒的 Python 爬虫工具来获取你需要的数据。</div></article><article class="list__item post"><figure class="list__thumbnail thumbnail"><a class=thumbnail__link href=/article-8267-1.html><img class=thumbnail__image src=/data/attachment/album/201703/04/160319vrfrrsl2x2lrlpzp.jpg.thumb.jpg alt="一个使用 asyncio 协程的网络爬虫（三）"></a></figure><header class=list__header><h2 class="list__title post__title"><a href=/article-8267-1.html rel=bookmark>一个使用 asyncio 协程的网络爬虫（三）</a></h2><div class="list__meta meta"><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-03-06T10:31:00Z>March 06, 2017</time></div></div></header><div class="content list__excerpt post__content clearfix">在最后一个阶段，我们将使用 Python 标准库“asyncio”中功能完整的协程， 并通过异步队列完成这个网络爬虫。</div></article><article class="list__item post"><figure class="list__thumbnail thumbnail"><a class=thumbnail__link href=/article-8266-1.html><img class=thumbnail__image src=/data/attachment/album/201703/04/160254v6p6n6aae1ywd5xz.jpg.thumb.jpg alt="一个使用 asyncio 协程的网络爬虫（二）"></a></figure><header class=list__header><h2 class="list__title post__title"><a href=/article-8266-1.html rel=bookmark>一个使用 asyncio 协程的网络爬虫（二）</a></h2><div class="list__meta meta"><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-03-05T10:27:00Z>March 05, 2017</time></div></div></header><div class="content list__excerpt post__content clearfix">然后，由于 Python 的协程不仅有效而且可扩展，我们将用 Python 的生成器函数实现一个简单的协程。</div></article><article class="list__item post"><figure class="list__thumbnail thumbnail"><a class=thumbnail__link href=/article-8265-1.html><img class=thumbnail__image src=/data/attachment/album/201703/04/160052krxzwz2jepx15pxr.jpg.thumb.jpg alt="一个使用 asyncio 协程的网络爬虫（一）"></a></figure><header class=list__header><h2 class="list__title post__title"><a href=/article-8265-1.html rel=bookmark>一个使用 asyncio 协程的网络爬虫（一）</a></h2><div class="list__meta meta"><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2017-03-04T15:59:00Z>March 04, 2017</time></div></div></header><div class="content list__excerpt post__content clearfix">首先，我们会实现一个事件循环并用这个事件循环和回调来勾画出一只网络爬虫。它很有效，但是当把它扩展成更复杂的问题时，就会导致无法管理的混乱代码。</div></article><article class="list__item post"><figure class="list__thumbnail thumbnail"><a class=thumbnail__link href=/article-7664-1.html><img class=thumbnail__image src=/data/attachment/album/201608/09/082037ijkkjwctadvpk6uc.jpg.thumb.jpg alt="Python 学习：urllib 简介"></a></figure><header class=list__header><h2 class="list__title post__title"><a href=/article-7664-1.html rel=bookmark>Python 学习：urllib 简介</a></h2><div class="list__meta meta"><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2016-08-09T08:20:00Z>August 09, 2016</time></div></div></header><div class="content list__excerpt post__content clearfix">Python 3 的 urllib 模块是一堆可以处理 URL 的组件集合。如果你有 Python 2 的知识，那么你就会注意到 Python 2 中有 urllib 和 urllib2 两个版本的模块。这些现在都是 Python 3 的 urllib 包的一部分。</div></article><article class="list__item post"><figure class="list__thumbnail thumbnail"><a class=thumbnail__link href=/article-5487-1.html><img class=thumbnail__image src=/data/attachment/album/201505/21/230248mh6hqj5x770q61ni.jpg.thumb.jpg alt=Linux有问必答：nginx网络服务器上如何阻止特定用户代理（UA）></a></figure><header class=list__header><h2 class="list__title post__title"><a href=/article-5487-1.html rel=bookmark>Linux有问必答：nginx网络服务器上如何阻止特定用户代理（UA）</a></h2><div class="list__meta meta"><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2015-05-22T08:30:00Z>May 22, 2015</time></div></div></header><div class="content list__excerpt post__content clearfix">问题： 我注意到有一些机器人经常访问我的nginx驱动的网站，并且进行一些攻击性的扫描，导致消耗掉了我的网络服务器的大量资源。我一直尝试着通过用户代理符串来阻挡这些机器人。我怎样才能在nginx网络服务器上阻挡掉特定的用户代理呢？ 现代互联网滋生了大量各种各样的恶意机器人和网络爬虫，比如像恶意软件机器人、垃圾邮件程序或内容刮刀，这些恶意工具一直偷偷摸摸地扫描你的网站，干些诸如检测潜在网站漏洞、收获电子邮件地址，或者只是从你的网站偷取内容。大多数机器人能够通过它们的用户代理签名字符串来识别。 作为第一道防线，</div></article><article class="list__item post"><figure class="list__thumbnail thumbnail"><a class=thumbnail__link href=/article-5098-1.html><img class=thumbnail__image src=/data/attachment/album/201503/21/174213pc6aqwfzf81qnzga.jpg.thumb.jpg alt="如何在Ubuntu 14.04 LTS安装网络爬虫工具：Scrapy"></a></figure><header class=list__header><h2 class="list__title post__title"><a href=/article-5098-1.html rel=bookmark>如何在Ubuntu 14.04 LTS安装网络爬虫工具：Scrapy</a></h2><div class="list__meta meta"><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2015-03-21T17:42:00Z>March 21, 2015</time></div></div></header><div class="content list__excerpt post__content clearfix">这是一款提取网站数据的开源工具。Scrapy框架用Python开发而成，它使抓取工作又快又简单，且可扩展。我们已经在virtual box中创建一台虚拟机（VM）并且在上面安装了Ubuntu 14.04 LTS。 安装 Scrapy Scrapy依赖于Python、开发库和pip。Python最新的版本已经在Ubuntu上预装了。因此我们在安装Scrapy之前只需安装pip和python开发库就可以了。 pip是作为python包索引器easy_install的替代品，用于安装和管理Python包。pip包的安装可见图 1。 sudo apt-get install python-pip 图:1 pip安装 我们必须要用下面的命令安装python开发库。如果包</div></article></main></div></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2025 Linux.cn Archive.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script></body></html>