<!doctype html><html class=no-js lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>用 Spark SQL 进行结构化数据处理 - Linux.cn Archive</title>
<script>(function(e,t){e[t]=e[t].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="用 Spark SQL 进行结构化数据处理"><meta property="og:description" content="Spark SQL 是 Spark 生态系统中处理结构化格式数据的模块。它在内部使用 Spark Core API 进行处理，但对用户的使用进行了抽象。这篇文章深入浅出地告诉你 Spark SQL 3.x 的新内容。"><meta property="og:type" content="article"><meta property="og:url" content="/article-14631-1.html"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-05-24T09:30:38+00:00"><meta property="article:modified_time" content="2022-05-24T09:30:38+00:00"><meta itemprop=name content="用 Spark SQL 进行结构化数据处理"><meta itemprop=description content="Spark SQL 是 Spark 生态系统中处理结构化格式数据的模块。它在内部使用 Spark Core API 进行处理，但对用户的使用进行了抽象。这篇文章深入浅出地告诉你 Spark SQL 3.x 的新内容。"><meta itemprop=datePublished content="2022-05-24T09:30:38+00:00"><meta itemprop=dateModified content="2022-05-24T09:30:38+00:00"><meta itemprop=wordCount content="183"><meta itemprop=keywords content="Spark,SQL,"><link rel=preconnect href=https://fonts.bunny.net crossorigin><link rel=dns-prefetch href=//fonts.bunny.net><link rel=dns-prefetch href=//fonts.bunny.net><link rel=stylesheet href="https://fonts.bunny.net/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel="shortcut icon" href=/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title="Linux.cn Archive" rel=home><div class="logo__item logo__text"><div class=logo__title>Linux.cn Archive</div></div></a></div><nav class=menu><button class=menu__btn aria-haspopup=true aria-expanded=false tabindex=0>
<span class=menu__btn-title tabindex=-1>Menu</span></button><ul class=menu__list><li class=menu__item><a class=menu__link href=/><span class=menu__text>Home</span></a></li><li class=menu__item><a class=menu__link href=/categories/><span class=menu__text>Categories</span></a></li><li class=menu__item><a class=menu__link href=/tags/><span class=menu__text>Tags</span></a></li><li class=menu__item><a class=menu__link href><span class=menu__text>RSS</span></a></li></ul></nav></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title>用 Spark SQL 进行结构化数据处理</h1><div class="post__meta meta"><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2022-05-24T09:30:38Z>May 24, 2022</time></div></div></header><div class="content post__content clearfix"><blockquote><p>Spark SQL 是 Spark 生态系统中处理结构化格式数据的模块。它在内部使用 Spark Core API 进行处理，但对用户的使用进行了抽象。这篇文章深入浅出地告诉你 Spark SQL 3.x 的新内容。</p></blockquote><p><img src=/data/attachment/album/202205/24/093036xaf6kaz1auaf4a7s.jpg alt></p><p>有了 Spark SQL，用户可以编写 SQL 风格的查询。这对于精通结构化查询语言或 SQL 的广大用户群体来说，基本上是很有帮助的。用户也将能够在结构化数据上编写交互式和临时性的查询。Spark SQL 弥补了 弹性分布式数据集 resilient distributed data sets （RDD）和关系表之间的差距。RDD 是 Spark 的基本数据结构。它将数据作为分布式对象存储在适合并行处理的节点集群中。RDD 很适合底层处理，但在运行时很难调试，程序员不能自动推断 模式 schema 。另外，RDD 没有内置的优化功能。Spark SQL 提供了 数据帧 DataFrame 和数据集来解决这些问题。</p><p>Spark SQL 可以使用现有的 Hive 元存储、SerDes 和 UDF。它可以使用 JDBC/ODBC 连接到现有的 BI 工具。</p><h3 id=数据源>数据源</h3><p>大数据处理通常需要处理不同的文件类型和数据源（关系型和非关系型）的能力。Spark SQL 支持一个统一的数据帧接口来处理不同类型的源，如下所示。</p><ul><li>文件：<ul><li>CSV</li><li>Text</li><li>JSON</li><li>XML</li></ul></li><li>JDBC/ODBC：<ul><li>MySQL</li><li>Oracle</li><li>Postgres</li></ul></li><li>带模式的文件：<ul><li>AVRO</li><li>Parquet</li></ul></li><li>Hive 表：<ul><li>Spark SQL 也支持读写存储在 Apache Hive 中的数据。</li></ul></li></ul><p>通过数据帧，用户可以无缝地读取这些多样化的数据源，并对其进行转换/连接。</p><h3 id=spark-sql-3x-的新内容>Spark SQL 3.x 的新内容</h3><p>在以前的版本中（Spark 2.x），查询计划是基于启发式规则和成本估算的。从解析到逻辑和物理查询计划，最后到优化的过程是连续的。这些版本对转换和行动的运行时特性几乎没有可见性。因此，由于以下原因，查询计划是次优的：</p><ul><li>缺失和过时的统计数据</li><li>次优的启发式方法</li><li>错误的成本估计</li></ul><p>Spark 3.x 通过使用运行时数据来迭代改进查询计划和优化，增强了这个过程。前一阶段的运行时统计数据被用来优化后续阶段的查询计划。这里有一个反馈回路，有助于重新规划和重新优化执行计划。</p><p><img src=/data/attachment/album/202205/24/093039bgv1g1kw54xbk211.jpg alt="Figure 1: Query planning"></p><h4 id=自适应查询执行aqe>自适应查询执行（AQE）</h4><p>查询被改变为逻辑计划，最后变成物理计划。这里的概念是“重新优化”。它利用前一阶段的可用数据，为后续阶段重新优化。正因为如此，整个查询的执行要快得多。</p><p>AQE 可以通过设置 SQL 配置来启用，如下所示（Spark 3.0 中默认为 false）：</p><pre tabindex=0><code>spark.conf.set(“spark.sql.adaptive.enabled”,true)
</code></pre><h4 id=动态合并洗牌分区>动态合并“洗牌”分区</h4><p>Spark 在“ 洗牌 shuffle ”操作后确定最佳的分区数量。在 AQE 中，Spark 使用默认的分区数，即 200 个。这可以通过配置来启用。</p><pre tabindex=0><code>spark.conf.set(“spark.sql.adaptive.coalescePartitions.enabled”,true)
</code></pre><h4 id=动态切换连接策略>动态切换连接策略</h4><p>广播哈希是最好的连接操作。如果其中一个数据集很小，Spark 可以动态地切换到广播连接，而不是在网络上“洗牌”大量的数据。</p><h4 id=动态优化倾斜连接>动态优化倾斜连接</h4><p>如果数据分布不均匀，数据会出现倾斜，会有一些大的分区。这些分区占用了大量的时间。Spark 3.x 通过将大分区分割成多个小分区来进行优化。这可以通过设置来启用：</p><pre tabindex=0><code>spark.conf.set(“spark.sql.adaptive.skewJoin.enabled”,true)
</code></pre><p><img src=/data/attachment/album/202205/24/093039mz91rb31jiyt3qjc.jpg alt="Figure 2: Performance improvement in Spark 3.x (Source: Databricks)"></p><h3 id=其他改进措施>其他改进措施</h3><p>此外，Spark SQL 3.x还支持以下内容。</p><h4 id=动态分区修剪>动态分区修剪</h4><p>3.x 将只读取基于其中一个表的值的相关分区。这消除了解析大表的需要。</p><h4 id=连接提示>连接提示</h4><p>如果用户对数据有了解，这允许用户指定要使用的连接策略。这增强了查询的执行过程。</p><h4 id=兼容-ansi-sql>兼容 ANSI SQL</h4><p>在兼容 Hive 的早期版本的 Spark 中，我们可以在查询中使用某些关键词，这样做是完全可行的。然而，这在 Spark SQL 3 中是不允许的，因为它有完整的 ANSI SQL 支持。例如，“将字符串转换为整数”会在运行时产生异常。它还支持保留关键字。</p><h4 id=较新的-hadoopjava-和-scala-版本>较新的 Hadoop、Java 和 Scala 版本</h4><p>从 Spark 3.0 开始，支持 Java 11 和 Scala 2.12。 Java 11 具有更好的原生协调和垃圾校正，从而带来更好的性能。 Scala 2.12 利用了 Java 8 的新特性，优于 2.11。</p><p>Spark 3.x 提供了这些现成的有用功能，而无需开发人员操心。这将显着提高 Spark 的整体性能。</p><hr><p>via: <a href=https://www.opensourceforu.com/2022/05/structured-data-processing-with-spark-sql/>https://www.opensourceforu.com/2022/05/structured-data-processing-with-spark-sql/</a></p><p>作者：<a href=https://www.opensourceforu.com/author/phani-kiran/>Phani Kiran</a> 选题：<a href=https://github.com/lkxed>lkxed</a> 译者：<a href=https://github.com/geekpi>geekpi</a> 校对：<a href=https://github.com/wxy>wxy</a></p><p>本文由 <a href=https://github.com/LCTT/TranslateProject>LCTT</a> 原创编译，<a href=https://linux.cn/>Linux中国</a> 荣誉推出</p></div><footer class=post__footer><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5.0 11V3C0 1.5.8.8.8.8S1.5.0 3 0h8c1.5.0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 100-6 3 3 0 000 6z"/></svg><ul class=tags__list><li class=tags__item><a class="tags__link btn" href=/tags/spark/ rel=tag>Spark</a></li><li class=tags__item><a class="tags__link btn" href=/tags/sql/ rel=tag>SQL</a></li></ul></div></footer></article></main></div></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2025 Linux.cn Archive.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script></body></html>