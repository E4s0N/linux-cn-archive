<!doctype html><html class=no-js lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>用 Python 轻松实现机器学习 - Linux.cn Archive</title>
<script>(function(e,t){e[t]=e[t].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="用 Python 轻松实现机器学习"><meta property="og:description" content="用朴素贝叶斯分类器解决现实世界里的机器学习问题。"><meta property="og:type" content="article"><meta property="og:url" content="/article-13628-1.html"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-07-29T10:50:41+00:00"><meta property="article:modified_time" content="2021-07-29T10:50:41+00:00"><meta itemprop=name content="用 Python 轻松实现机器学习"><meta itemprop=description content="用朴素贝叶斯分类器解决现实世界里的机器学习问题。"><meta itemprop=datePublished content="2021-07-29T10:50:41+00:00"><meta itemprop=dateModified content="2021-07-29T10:50:41+00:00"><meta itemprop=wordCount content="512"><meta itemprop=keywords content="贝叶斯,机器学习,"><link rel=preconnect href=https://fonts.bunny.net crossorigin><link rel=dns-prefetch href=//fonts.bunny.net><link rel=dns-prefetch href=//fonts.bunny.net><link rel=stylesheet href="https://fonts.bunny.net/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel="shortcut icon" href=/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title="Linux.cn Archive" rel=home><div class="logo__item logo__text"><div class=logo__title>Linux.cn Archive</div></div></a></div><nav class=menu><button class=menu__btn aria-haspopup=true aria-expanded=false tabindex=0>
<span class=menu__btn-title tabindex=-1>Menu</span></button><ul class=menu__list><li class=menu__item><a class=menu__link href=/><span class=menu__text>Home</span></a></li><li class=menu__item><a class=menu__link href=/categories/><span class=menu__text>Categories</span></a></li><li class=menu__item><a class=menu__link href=/tags/><span class=menu__text>Tags</span></a></li><li class=menu__item><a class=menu__link href><span class=menu__text>RSS</span></a></li></ul></nav></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title>用 Python 轻松实现机器学习</h1><div class="post__meta meta"><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2021-07-29T10:50:41Z>July 29, 2021</time></div></div></header><div class="content post__content clearfix"><blockquote><p>用朴素贝叶斯分类器解决现实世界里的机器学习问题。</p></blockquote><p><img src=https://img.linux.net.cn/data/attachment/album/202107/29/105035ocxdhdob78wmmtzd.jpg alt title="arrows cycle symbol for failing faster"></p><p>朴素贝叶斯 Naïve Bayes 是一种分类技术，它是许多分类器建模算法的基础。基于朴素贝叶斯的分类器是简单、快速和易用的机器学习技术之一，而且在现实世界的应用中很有效。</p><p>朴素贝叶斯是从 贝叶斯定理 Bayes&rsquo; theorem 发展来的。贝叶斯定理由 18 世纪的统计学家 <a href=https://en.wikipedia.org/wiki/Thomas_Bayes>托马斯·贝叶斯</a> 提出，它根据与一个事件相关联的其他条件来计算该事件发生的概率。比如，<a href=https://en.wikipedia.org/wiki/Parkinson%27s_disease>帕金森氏病</a> 患者通常嗓音会发生变化，因此嗓音变化就是与预测帕金森氏病相关联的症状。贝叶斯定理提供了计算目标事件发生概率的方法，而朴素贝叶斯是对该方法的推广和简化。</p><h3 id=解决一个现实世界里的问题>解决一个现实世界里的问题</h3><p>这篇文章展示了朴素贝叶斯分类器解决现实世界问题（相对于完整的商业级应用）的能力。我会假设你对机器学习有基本的了解，所以文章里会跳过一些与机器学习预测不大相关的步骤，比如 数据打乱 date shuffling 和 数据切片 data splitting 。如果你是机器学习方面的新手或者需要一个进修课程，请查看 《<a href=https://opensource.com/article/17/9/introduction-machine-learning>An introduction to machine learning today</a>》 和 《<a href=https://opensource.com/business/15/9/getting-started-open-source-machine-learning>Getting started with open source machine learning</a>》。</p><p>朴素贝叶斯分类器是 有监督的 supervised 、属于 生成模型 generative 的、非线性的、属于 参数模型 parametric 的和 基于概率的 probabilistic 。</p><p>在这篇文章里，我会演示如何用朴素贝叶斯预测帕金森氏病。需要用到的数据集来自 <a href=https://archive.ics.uci.edu/ml/datasets/parkinsons>UCI 机器学习库</a>。这个数据集包含许多语音信号的指标，用于计算患帕金森氏病的可能性；在这个例子里我们将使用这些指标中的前 8 个：</p><ul><li><strong>MDVP:Fo(Hz)</strong>：平均声带基频</li><li><strong>MDVP:Fhi(Hz)</strong>：最高声带基频</li><li><strong>MDVP:Flo(Hz)</strong>：最低声带基频</li><li><strong>MDVP:Jitter(%)</strong>、<strong>MDVP:Jitter(Abs)</strong>、<strong>MDVP:RAP</strong>、<strong>MDVP:PPQ</strong> 和 <strong>Jitter:DDP</strong>：5 个衡量声带基频变化的指标</li></ul><p>这个例子里用到的数据集，可以在我的 <a href=https://github.com/gammay/Machine-learning-made-easy-Naive-Bayes/tree/main/parkinsons>GitHub 仓库</a> 里找到。数据集已经事先做了打乱和切片。</p><h3 id=用-python-实现机器学习>用 Python 实现机器学习</h3><p>接下来我会用 Python 来解决这个问题。我用的软件是：</p><ul><li>Python 3.8.2</li><li>Pandas 1.1.1</li><li>scikit-learn 0.22.2.post1</li></ul><p>Python 有多个朴素贝叶斯分类器的实现，都是开源的，包括：</p><ul><li><strong>NLTK Naïve Bayes</strong>：基于标准的朴素贝叶斯算法，用于文本分类</li><li><strong>NLTK Positive Naïve Bayes</strong>：NLTK Naïve Bayes 的变体，用于对只标注了一部分的训练集进行二分类</li><li><strong>Scikit-learn Gaussian Naïve Bayes</strong>：提供了部分拟合方法来支持数据流或很大的数据集（LCTT 译注：它们可能无法一次性导入内存，用部分拟合可以动态地增加数据）</li><li><strong>Scikit-learn Multinomial Naïve Bayes</strong>：针对离散型特征、实例计数、频率等作了优化</li><li><strong>Scikit-learn Bernoulli Naïve Bayes</strong>：用于各个特征都是二元变量/布尔特征的情况</li></ul><p>在这个例子里我将使用 <a href=https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html>sklearn Gaussian Naive Bayes</a>。</p><p>我的 Python 实现在 <code>naive_bayes_parkinsons.py</code> 里，如下所示：</p><pre tabindex=0><code>import pandas as pd

# x_rows 是我们所使用的 8 个特征的列名
x_rows=[&#39;MDVP:Fo(Hz)&#39;,&#39;MDVP:Fhi(Hz)&#39;,&#39;MDVP:Flo(Hz)&#39;,
        &#39;MDVP:Jitter(%)&#39;,&#39;MDVP:Jitter(Abs)&#39;,&#39;MDVP:RAP&#39;,&#39;MDVP:PPQ&#39;,&#39;Jitter:DDP&#39;]
y_rows=[&#39;status&#39;] # y_rows 是类别的列名，若患病，值为 1，若不患病，值为 0

# 训练

# 读取训练数据
train_data = pd.read_csv(&#39;parkinsons/Data_Parkinsons_TRAIN.csv&#39;)
train_x = train_data[x_rows]
train_y = train_data[y_rows]
print(&#34;train_x:\n&#34;, train_x)
print(&#34;train_y:\n&#34;, train_y)

# 导入 sklearn Gaussian Naive Bayes，然后进行对训练数据进行拟合
from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()
gnb.fit(train_x, train_y)

# 对训练数据进行预测
predict_train = gnb.predict(train_x)
print(&#39;Prediction on train data:&#39;, predict_train)

# 在训练数据上的准确率
from sklearn.metrics import accuracy_score
accuracy_train = accuracy_score(train_y, predict_train)
print(&#39;Accuray score on train data:&#39;, accuracy_train)

# 测试

# 读取测试数据
test_data = pd.read_csv(&#39;parkinsons/Data_Parkinsons_TEST.csv&#39;)
test_x = test_data[x_rows]
test_y = test_data[y_rows]

# 对测试数据进行预测
predict_test = gnb.predict(test_x)
print(&#39;Prediction on test data:&#39;, predict_test)

# 在测试数据上的准确率
accuracy_test = accuracy_score(test_y, predict_test)
print(&#39;Accuray score on test data:&#39;, accuracy_train)
</code></pre><p>运行这个 Python 脚本：</p><pre tabindex=0><code>$ python naive_bayes_parkinsons.py

train_x:
      MDVP:Fo(Hz)  MDVP:Fhi(Hz) ...  MDVP:RAP  MDVP:PPQ  Jitter:DDP
0        152.125       161.469  ...   0.00191   0.00226     0.00574
1        120.080       139.710  ...   0.00180   0.00220     0.00540
2        122.400       148.650  ...   0.00465   0.00696     0.01394
3        237.323       243.709  ...   0.00173   0.00159     0.00519
..           ...           ...           ...  ...       ...       ...        
155      138.190       203.522  ...   0.00406   0.00398     0.01218

[156 rows x 8 columns]

train_y:
      status
0         1
1         1
2         1
3         0
..      ...
155       1

[156 rows x 1 columns]

Prediction on train data: [1 1 1 0 ... 1]
Accuracy score on train data: 0.6666666666666666

Prediction on test data: [1 1 1 1 ... 1
 1 1]
Accuracy score on test data: 0.6666666666666666
</code></pre><p>在训练集和测试集上的准确率都是 67%。它的性能还可以进一步优化。你想尝试一下吗？你可以在下面的评论区给出你的方法。</p><h3 id=背后原理>背后原理</h3><p>朴素贝叶斯分类器从贝叶斯定理发展来的。贝叶斯定理用于计算条件概率，或者说贝叶斯定理用于计算当与一个事件相关联的其他事件发生时，该事件发生的概率。简而言之，它解决了这个问题：<em>如果我们已经知道事件 x 发生在事件 y 之前的概率，那么当事件 x 再次发生时，事件 y 发生的概率是多少？</em> 贝叶斯定理用一个先验的预测值来逐渐逼近一个最终的 <a href=https://en.wikipedia.org/wiki/Posterior_probability>后验概率</a>。贝叶斯定理有一个基本假设，就是所有的参数重要性相同（LCTT 译注：即相互独立）。</p><p>贝叶斯计算主要包括以下步骤：</p><ol><li>计算总的先验概率：<br>P(患病)P(患病) 和 P(不患病)P(不患病)</li><li>计算 8 种指标各自是某个值时的后验概率 (value1,&mldr;,value8 分别是 MDVP:Fo(Hz)，&mldr;，Jitter:DDP 的取值)：<br>P(value1,\ldots,value8\ |\ 患病)P(value1,…,value8 ∣ 患病)<br>P(value1,\ldots,value8\ |\ 不患病)P(value1,…,value8 ∣ 不患病)</li><li>将第 1 步和第 2 步的结果相乘，最终得到患病和不患病的后验概率：<br>P(患病\ |\ value1,\ldots,value8) \propto P(患病) \times P(value1,\ldots,value8\ |\ 患病)P(患病 ∣ value1,…,value8)∝P(患病)×P(value1,…,value8 ∣ 患病)<br>P(不患病\ |\ value1,\ldots,value8) \propto P(不患病) \times P(value1,\ldots,value8\ |\ 不患病)P(不患病 ∣ value1,…,value8)∝P(不患病)×P(value1,…,value8 ∣ 不患病)</li></ol><p>上面第 2 步的计算非常复杂，朴素贝叶斯将它作了简化：</p><ol><li>计算总的先验概率：<br>P(患病)P(患病) 和 P(不患病)P(不患病)</li><li>对 8 种指标里的每个指标，计算其取某个值时的后验概率：<br>P(value1\ |\ 患病),\ldots,P(value8\ |\ 患病)P(value1 ∣ 患病),…,P(value8 ∣ 患病)<br>P(value1\ |\ 不患病),\ldots,P(value8\ |\ 不患病)P(value1 ∣ 不患病),…,P(value8 ∣ 不患病)</li><li>将第 1 步和第 2 步的结果相乘，最终得到患病和不患病的后验概率：<br>P(患病\ |\ value1,\ldots,value8) \propto P(患病) \times P(value1\ |\ 患病) \times \ldots \times P(value8\ |\ 患病)P(患病 ∣ value1,…,value8)∝P(患病)×P(value1 ∣ 患病)×…×P(value8 ∣ 患病)<br>P(不患病\ |\ value1,\ldots,value8) \propto P(不患病) \times P(value1\ |\ 不患病) \times \ldots \times P(value8\ |\ 不患病)P(不患病 ∣ value1,…,value8)∝P(不患病)×P(value1 ∣ 不患病)×…×P(value8 ∣ 不患病)</li></ol><p>这只是一个很初步的解释，还有很多其他因素需要考虑，比如数据类型的差异，稀疏数据，数据可能有缺失值等。</p><h3 id=超参数>超参数</h3><p>朴素贝叶斯作为一个简单直接的算法，不需要超参数。然而，有的版本的朴素贝叶斯实现可能提供一些高级特性（比如超参数）。比如，<a href=https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html>GaussianNB</a> 就有 2 个超参数：</p><ul><li><strong>priors</strong>：先验概率，可以事先指定，这样就不必让算法从数据中计算才能得出。</li><li><strong>var_smoothing</strong>：考虑数据的分布情况，当数据不满足标准的高斯分布时，这个超参数会发挥作用。</li></ul><h3 id=损失函数>损失函数</h3><p>为了坚持简单的原则，朴素贝叶斯使用 <a href=https://en.wikipedia.org/wiki/Loss_function#0-1_loss_function>0-1 损失函数</a>。如果预测结果与期望的输出相匹配，损失值为 0，否则为 1。</p><h3 id=优缺点>优缺点</h3><p><strong>优点</strong>：朴素贝叶斯是最简单、最快速的算法之一。<br><strong>优点</strong>：在数据量较少时，用朴素贝叶斯仍可作出可靠的预测。<br><strong>缺点</strong>：朴素贝叶斯的预测只是估计值，并不准确。它胜在速度而不是准确度。<br><strong>缺点</strong>：朴素贝叶斯有一个基本假设，就是所有特征相互独立，但现实情况并不总是如此。</p><p>从本质上说，朴素贝叶斯是贝叶斯定理的推广。它是最简单最快速的机器学习算法之一，用来进行简单和快速的训练和预测。朴素贝叶斯提供了足够好、比较准确的预测。朴素贝叶斯假设预测特征之间是相互独立的。已经有许多朴素贝叶斯的开源的实现，它们的特性甚至超过了贝叶斯算法的实现。</p><hr><p>via: <a href=https://opensource.com/article/21/1/machine-learning-python>https://opensource.com/article/21/1/machine-learning-python</a></p><p>作者：<a href=https://opensource.com/users/gammay>Girish Managoli</a> 选题：<a href=https://github.com/lujun9972>lujun9972</a> 译者：<a href=https://github.com/tanloong>tanloong</a> 校对：<a href=https://github.com/wxy>wxy</a></p><p>本文由 <a href=https://github.com/LCTT/TranslateProject>LCTT</a> 原创编译，<a href=https://linux.cn/>Linux中国</a> 荣誉推出</p></div><footer class=post__footer><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5.0 11V3C0 1.5.8.8.8.8S1.5.0 3 0h8c1.5.0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 100-6 3 3 0 000 6z"/></svg><ul class=tags__list><li class=tags__item><a class="tags__link btn" href=/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF/ rel=tag>贝叶斯</a></li><li class=tags__item><a class="tags__link btn" href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ rel=tag>机器学习</a></li></ul></div></footer></article></main></div></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2025 Linux.cn Archive.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script></body></html>